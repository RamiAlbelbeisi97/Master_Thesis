{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw96lUzwtJk1"
   },
   "source": [
    "# Feature extraction and reverse image search using pre-trained Deep Convolutional Neural Networks\n",
    "\n",
    "This notebook deals with the procedure of analyzing a large set of images using a pre-trained convolutional network, extracting feature vectors (activations of last layer) for each one which represent each image. \n",
    "\n",
    "After the analysis is done, we will review some retrieval tasks that you can do with such an analysis. The main task will be that of \"reverse image search,\" which refers to searching for the most similar set of images to some query image. \n",
    "\n",
    "\n",
    "### Step 0: Make a GPU-enabled environment (via the terminal, before runnning python code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) astro   2) cuda/11.2   3) python/anaconda3/2021.05\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's important to do pip install keras and not conda install keras because otherwise it would downgrade the tensorflow module.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!module list\n",
    "################################WITH CONDA\n",
    "\n",
    "#1 step create environment\n",
    "# conda -n gputest python=3\n",
    "\n",
    "#2 activate environment\n",
    "#conda activate gputest\n",
    "\n",
    "#3 install kernel for the gpu enbaling (?!)\n",
    "# pip install ipykernel\n",
    "\n",
    "#4 set kernel\n",
    "# python -m ipykernel install --user --name gputest --display-name \"gputest\"\n",
    "\n",
    "#5 get tensorflow for gpu running\n",
    "#conda install tensorflow-gpu\n",
    "\n",
    "#6 install jupyter environment\n",
    "#conda install jupyter\n",
    "\n",
    "# install keras library\n",
    "#pip install keras \n",
    "\n",
    "'''It's important to do pip install keras and \\\n",
    "not conda install keras because otherwise it would downgrade the \\\n",
    "tensorflow module.'''\n",
    "# 7 run the notebook\n",
    "#jupyter notebook\n",
    "\n",
    "\n",
    "#################################### WITH virtualenv\n",
    "\n",
    "\n",
    "#1  Build environment\n",
    "#virtualenv --system-site-packages targetDirectory \n",
    "#2 Activate environment\n",
    "# source ~/targetDirectory/bin/activate\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Import necessary modules and write main functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 22:46:56.472585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image \n",
    "import time\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "# config = tensorflow.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tensorflow.Session(config=config)\n",
    "\n",
    "\n",
    "\n",
    "def conv_model(model_name='resnet',show_model=True):\n",
    "    if model_name=='inception_v3': #works\n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        model = tensorflow.keras.applications.InceptionV3(weights='imagenet', include_top=True)\n",
    "    elif model_name=='vgg16':   #works \n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        model = tensorflow.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "    elif model_name=='xception': #works\n",
    "        from keras.applications.xception import Xception\n",
    "        model = tensorflow.keras.applications.Xception(weights='imagenet', include_top=True)\n",
    "    elif model_name=='resnet': #doesn't work\n",
    "        from keras.applications.resnet50 import ResNet50\n",
    "        model = tensorflow.keras.applications.ResNet50(weights='imagenet', include_top=True)\n",
    "    elif model_name=='vgg': #works\n",
    "        model = tensorflow.keras.applications.VGG19(weights='imagenet', include_top=True)\n",
    "    elif model_name=='MobileNet': #doesn't work\n",
    "        from keras.applications.mobilenet import MobileNet\n",
    "        model = tensorflow.keras.applications.MobileNet(weights='imagenet', include_top=True)\n",
    "    else:\n",
    "        print('Incorrect model_name fed to function conv_model')\n",
    "    if model!=None:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable=False\n",
    "    if show_model==True:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def delete_model(model, clear_session=True):\n",
    "    '''removes model!\n",
    "    '''\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if clear_session: K.clear_session()\n",
    "\n",
    "def get_neighbors(feature_vector,feature_vectors, k=5):\n",
    "    '''Function that returns the neighbor distances and their indeces (of the image array corresponding to the images names)'''\n",
    "    similar_idx = [ distance.euclidean(feature_vector, feat) for feat in feature_vectors ]\n",
    "    idx_closest = sorted(range(len(similar_idx)), key=lambda k: similar_idx[k])[1:1+k]\n",
    "    distances=[]\n",
    "    for i in idx_closest:\n",
    "        distances.append(similar_idx[i])\n",
    "    distances=np.array(distances)\n",
    "    idx_closest=np.array(idx_closest)\n",
    "    return distances,idx_closest        \n",
    "\n",
    "\n",
    "def load_image_v2(path,model_name='resnet'):\n",
    "    '''An effort to write a different function to load the model, since I get a bug when trying to load more than 500 pictures.'''\n",
    "    img = tf.keras.preprocessing.image.load_img(path, grayscale=False, color_mode='rgb', target_size=model.input_shape[1:3],interpolation='nearest')\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    x = np.array([input_arr])  # Convert single image to a batch\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    if model=='inception_v3':\n",
    "        from keras.applications.inception_v3 import preprocess_input as process_inception\n",
    "        x=preprocess_inception(x)\n",
    "    elif model_name=='vgg16':\n",
    "        from keras.applications.vgg16 import preprocess_input as process_vgg16\n",
    "        print('shape of vgg16 input:',np.shape(x))\n",
    "        x=process_vgg16(x)\n",
    "    elif model_name=='xception':\n",
    "        from keras.applications.xception import preprocess_input as process_xception\n",
    "        x=process_xception(x)\n",
    "    elif model_name=='resnet':\n",
    "        from keras_applications.resnet import preprocess_input as process_resnet\n",
    "        print('shape of resnet input:',np.shape(x))\n",
    "        x=process_resnet(x)\n",
    "    elif model_name=='vgg':\n",
    "        from keras.applications.vgg19 import preprocess_input as process_vgg\n",
    "        x=process_vgg(x)\n",
    "    elif model_name=='mobile':\n",
    "        from keras.applications.mobilenet import preprocess_input as process_mobile\n",
    "        x=process_mobile(x)  \n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function load_image')\n",
    "    return img, x\n",
    "\n",
    "\n",
    "def load_image(path,model_name='resnet'):\n",
    "    '''Load an image an preprocess it, and return the image and the preprocessed input'''\n",
    "    img = image.load_img(path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    if model=='inception_v3':\n",
    "        from keras.applications.inception_v3 import preprocess_input as process_inception\n",
    "        x=preprocess_inception(x)\n",
    "    elif model_name=='vgg16':\n",
    "        from keras.applications.vgg16 import preprocess_input as process_vgg16\n",
    "        print('shape of vgg16 input:',np.shape(x))\n",
    "        x=process_vgg16(x)\n",
    "    elif model_name=='xception':\n",
    "        from keras.applications.xception import preprocess_input as process_xception\n",
    "        x=process_xception(x)\n",
    "    elif model_name=='resnet':\n",
    "        from keras_applications.resnet import preprocess_input as process_resnet\n",
    "        print('shape of resnet input:',np.shape(x))\n",
    "        x=process_resnet(x)\n",
    "    elif model_name=='vgg':\n",
    "        from keras.applications.vgg19 import preprocess_input as process_vgg\n",
    "        x=process_vgg(x)\n",
    "    elif model_name=='mobile':\n",
    "        from keras.applications.mobilenet import preprocess_input as process_mobile\n",
    "        x=process_mobile(x)  \n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function load_image')\n",
    "    return img, x\n",
    "\n",
    "def img_to_conv_features(model_name,model,x):\n",
    "    '''Convert 1 image to its convolutional features'''\n",
    "    if model_name=='inception_v3':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='vgg16':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "        feat = feat_extractor.predict(x)\n",
    "    elif model_name=='xception':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='resnet':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "        feat = feat_extractor.predict(x)\n",
    "    elif model_name=='vgg':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function img_to_conv_features')\n",
    "    feat=feat_extractor(x)\n",
    "    return feat\n",
    "def imgs_to_conv_features(model_name,model,images,image_path):\n",
    "    '''Convert a batch of images into its convolutional features'''\n",
    "    if model_name=='inception_v3':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='vgg16':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    elif model_name=='xception':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='resnet':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    elif model_name=='vgg':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    elif model_name=='MobileNet':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function img_to_conv_features')\n",
    "    tic = time.process_time()\n",
    "\n",
    "    every=20\n",
    "    features = []\n",
    "    for i, image_path in enumerate(images):\n",
    "\n",
    "        if i % every == 0:\n",
    "            toc = time.process_time()\n",
    "            elap = toc-tic;\n",
    "            remaining_time=(len(images)-i)*elap/every\n",
    "            hours=remaining_time//3600\n",
    "            minutes=remaining_time//60\n",
    "            seconds=remaining_time%60\n",
    "            print(\"analyzing image %d / %d. Time/%d pics: %4.4f seconds.\" % (i, len(images),every,elap))\n",
    "            print('Remaining time: %d hours %d minutes %d sec.' %(hours,minutes,seconds))\n",
    "            \n",
    "            tic = time.process_time()\n",
    "        img, x = load_image(path=image_path,model_name=model_name);\n",
    "        #print(image_path+\"\\n\")\n",
    "        feat = feat_extractor.predict(x,batch_size=32)[0]\n",
    "        features.append(feat)\n",
    "\n",
    "    print('finished extracting features for %d images' % len(images))\n",
    "    return features\n",
    "\n",
    "\n",
    "def choose_imgs(max_num_images=2000,images_path='C:\\\\Users\\\\Rami\\\\Desktop\\\\PetImages\\\\dogs-vs-cats'):\n",
    "    image_extensions = ['.jpg', '.png', '.jpeg']   # case-insensitive (upper/lower doesn't matter)\n",
    "    images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(images_path) for f in filenames if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    print(np.shape(images))\n",
    "    if max_num_images < len(images):\n",
    "        images = [images[i] for i in sorted(random.sample(range(len(images)), max_num_images))]\n",
    "\n",
    "    print(\"keeping %d images to analyze\" % len(images))\n",
    "    return images\n",
    "def reduce_PCA(features,n_components=40):\n",
    "    features = np.array(features)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(features)\n",
    "    pca_features = pca.transform(features)\n",
    "    return pca_features\n",
    "\n",
    "def plot_activation_layer(feature_vector):\n",
    "    \n",
    "    print(np.shape(feature_vector))\n",
    "    feature_vector=np.array(feature_vector)\n",
    "    feature_vector=feature_vector.flatten()\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.ylabel('Activations of last layer (fc2)')\n",
    "    plt.xlabel('## of neuron')\n",
    "    plt.plot(feature_vector)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def get_concatenated_images(indexes, thumb_height):\n",
    "    thumbs = []\n",
    "    for idx in indexes:\n",
    "        img = image.load_img(images[idx])\n",
    "        img = img.resize((int(img.width * thumb_height / img.height), thumb_height))\n",
    "        thumbs.append(img)\n",
    "    concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n",
    "    return concat_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check version of tensorflow and Default running device (GPU OR CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# from distutils.version import LooseVersion\n",
    "# import warnings\n",
    "\n",
    "# assert LooseVersion(tensorflow.__version__)>=LooseVersion('1.0'), 'Please use Tensorflow'\n",
    "# print('TensorFlow Version: {}'.format(tensorflow.__version__))\n",
    "\n",
    "# #Check for gpu\n",
    "# if not tensorflow.test.gpu_device_name():\n",
    "#     warnings.warn('No GPU found. Please ensure you have installed  TensorFlow correctly')\n",
    "# else: \n",
    "#     print('Default GPU Device: {}'.format(tensorflow.test.gpu_device_name()))\n",
    "    \n",
    "# device_name=tensorflow.test.gpu_device_name()\n",
    "# gpu_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs:\", len(gpu_devices))\n",
    "# cpu_devices = tensorflow.config.list_physical_devices('CPU')\n",
    "# print(\"Num CPUs:\", len(cpu_devices))\n",
    "\n",
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "## Test if tensorflow is built with gpu support\n",
    "# \n",
    "# print(tensorflow.test.is_built_with_gpu_support())\n",
    "\n",
    "#Get number of threads used for parallelism between independent operations.\n",
    "#tf.config.threading.get_inter_op_parallelism_threads()\n",
    "\n",
    "#Get number of threads used within an individual op for parallelism.\n",
    "#tf.config.threading.get_intra_op_parallelism_threads()\n",
    "\n",
    "#Determines the number of threads used by independent non-blocking operations. 0 means the system picks an appropriate number.\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(    num_threads)\n",
    "\n",
    "\n",
    "#Set number of threads used within an individual op for parallelism.\n",
    "#Certain operations like matrix multiplication and reductions can utilize parallel threads for speed ups. A value of 0 means the system picks an appropriate number.\n",
    "\n",
    "#tf.config.threading.set_intra_op_parallelism_threads(    num_threads)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for d in devices:\n",
    "#     t = d.device_type\n",
    "#     name = d.physical_device_desc\n",
    "#     l = [item.split(':',1) for item in name.split(\", \")]\n",
    "#     name_attr = dict([x for x in l if len(x)==2])\n",
    "#     dev = name_attr.get('name', 'Unnamed device')\n",
    "#     print(f\" {d.name} || {dev} || {t} || {sizeof_fmt(d.memory_limit)}\")\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "# GPUS = [\"GPU:0\"]\n",
    "\n",
    "# def process(image, label):\n",
    "#     image = tf.image.resize(image, [299, 299]) / 255.0\n",
    "#     return image, label\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy( GPUS )\n",
    "# print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "# batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "# dataset = dataset.map(process).shuffle(500).batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# start = time.time()\n",
    "# with strategy.scope():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to set which image we want to compare to which folder of images. We can also choose the model and get some useful messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit about the context\n",
    "\n",
    "The networks that are implemented are some of the top performing on the classification contest by imagenet on classifying  \n",
    "pictures on 1000 labels , when fed with a dataset 1.2 milion pictures. \n",
    "Since the machine learning consists of \n",
    "\n",
    "\n",
    "Step 1: Designing the architechture of your network\n",
    "\n",
    "\n",
    "(how many layers, how many neurons (units) does a layer have, which\n",
    "connections exist between adjacent layers, and what is the mathematical operation that a layer operates on the input in order\n",
    "to produce the output (activation function) which wiill be fed to the next layer.\n",
    "\n",
    "Step 2: Train the model\n",
    "\n",
    "  Start by initializing some numbers for all weight (for example set them all to 0), and then try to find optimal values\n",
    "for these parameters, the weights between the connections. The basic idea on how to find these optimal values,\n",
    "is to define a loss function which represents  how well the programm is achieving what we want it to do. We want the loss\n",
    "function to be small when the program is accomplishing the task (classifying images with labels correctly)\n",
    "and we want the loss to be large when the program is failing (you give it a picture of a cat, and it tells you its a dog).\n",
    "Since the loss function of the CNN is dependant by some mathematic expression to the weights of the network, minimizing is\n",
    "an optimization problem which can be solved by just walking towards the negative derivative of L(W1,w2..,W_n).\n",
    "A common algorithm that does this downhill walk on ths multidimensional space of weights in order\n",
    "to find the optimal values of the weights is stohastic gradient descent (SGD)\n",
    "\n",
    "\n",
    "Step 3: Your model is trained now and has small loss. \n",
    "\n",
    "Now the cnn can receive new input images and be able to produce \"signature-information\" containing feature vectors.\n",
    "\n",
    "\n",
    "Propagate the input x in the network and return the second-last layer, commonly called feature vector (denamed by \n",
    "  variable feature here). The feature vector has ~10^3 dimensions and simply consists of all the weight between the last 2 \n",
    "layers.  It is common in convolutional neural network (CNN) architecture that the two last layers are fully\n",
    "conected, while the rest of the layers inside the convolutional network (convolutions,max pool, avg pool, etc.) can skip\n",
    "connections from a layer to the next one.\n",
    "\n",
    "\n",
    "Remember that the networks that we are implementing have very high accuracy on classifying 1000 different types of objects\n",
    "which means that the layer before the last inputs some vector and outputs it's activation function to the last layer,\n",
    "which is the 1000 element layer of the predictions. Because of this, the feature vector is a better representation of the \n",
    "image's content than simply the 2D X3 (rgb) matrix of the colors consisting the image in order to assess a similarity measure.\n",
    "\n",
    "\n",
    "We dont use the last layer of the network (predictions) but the second last (feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to find the closest pictures could also be to say: let's find the pictures in the dataset which are the closest to our own by distance on their vector space. These ones with the least distance will be classified as best matches for the reverse image search. We will use the 5-nearest-neighbours algorithm, which in general for k neighbours is called kNN (k nearest neighbours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/astro/rami/tensorflow_gpu\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the neural network\n",
    "### All  the code in one cell without many outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running search with image:/groups/astro/rami/images/image_000005.png \n",
      " comparing to 20 images \n",
      " which are in folder /groups/astro/rami/images/, \n",
      " using model vgg \n",
      " and distance metric:knn with 5 nearest neighbors. \n",
      " PCA used?:False \n",
      " recalculating dataset?True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 22:48:45.121063: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-03 22:48:45.122030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-03 22:48:45.147679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:45.149118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:25:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:45.150526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:c1:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:45.151939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:e1:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:45.151956: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-03 22:48:45.155215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-03 22:48:45.155242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-10-03 22:48:45.504032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-03 22:48:45.642787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-03 22:48:46.133632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-03 22:48:46.358989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-03 22:48:46.363540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-03 22:48:46.374841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2021-10-03 22:48:46.375432: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 22:48:46.759704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:46.761143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:25:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:46.762548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:c1:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:46.763942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:e1:00.0 name: A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-10-03 22:48:46.763984: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-03 22:48:46.764023: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-03 22:48:46.764032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-10-03 22:48:46.764041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-03 22:48:46.764050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-03 22:48:46.764058: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-03 22:48:46.764066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-03 22:48:46.764075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-03 22:48:46.775034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2021-10-03 22:48:46.775092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-03 23:12:55.604491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-03 23:12:55.604525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 \n",
      "2021-10-03 23:12:55.604544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y \n",
      "2021-10-03 23:12:55.604547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y \n",
      "2021-10-03 23:12:55.604550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y \n",
      "2021-10-03 23:12:55.604553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N \n",
      "2021-10-03 23:12:55.613309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37077 MB memory) -> physical GPU (device: 0, name: A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0)\n",
      "2021-10-03 23:12:55.616524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 37013 MB memory) -> physical GPU (device: 1, name: A100-PCIE-40GB, pci bus id: 0000:25:00.0, compute capability: 8.0)\n",
      "2021-10-03 23:12:55.619437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 37077 MB memory) -> physical GPU (device: 2, name: A100-PCIE-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0)\n",
      "2021-10-03 23:12:55.622454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 36959 MB memory) -> physical GPU (device: 3, name: A100-PCIE-40GB, pci bus id: 0000:e1:00.0, compute capability: 8.0)\n",
      "2021-10-03 23:12:55.622593: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 143,667,240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 23:12:58.279584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "##########################################################\n",
    "#Set the path to the image, images and max number of imgs#\n",
    "path='/groups/astro/rami/images/image_000005.png'        #\n",
    "image_path='/groups/astro/rami/images/'                  #\n",
    "max_num_images=20                                      #\n",
    "#Which Deep Neural network to use                        #\n",
    "model_name='vgg'                                         #\n",
    "#How to compare the features(fingerprints)               #\n",
    "distance_metric='knn'                                    #\n",
    "k=5                                                      #\n",
    "#Reduce dimensionality of data?                          #\n",
    "use_PCA=False                                            # \n",
    "n_components=500                                         #\n",
    "                                                         #\n",
    "#True for calculating feature vectors                    #   \n",
    "#for the entire dataset                                  #\n",
    "recalculate_dataset=True                                 #\n",
    "#Show the output on jupyter?                             #\n",
    "display_output=True                                      #\n",
    "#Save the output somewhere?                              #\n",
    "save_output=False                                        #\n",
    "output_folder='/lustre/hpc/astro/rami/images/'           #\n",
    "                                                         #\n",
    "use_timing=True                                          #\n",
    "                                                         #\n",
    "                                                         #\n",
    "                                                         #\n",
    "                                                         #\n",
    "###########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running search with image:%s \\n comparing to %d images \\n which are in folder %s, \\n using model %s \\n and distance metric:\\\n",
    "%s with %d nearest neighbors. \\n PCA used?:%s \\n recalculating dataset?%s \"%(path,max_num_images,image_path,model_name,distance_metric,k,use_PCA,recalculate_dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Initializing model\n",
    "if use_timing:\n",
    "    t0 = time.time()\n",
    "model=conv_model(model_name=model_name)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculating target image (observation) N.N.-fingerprint\n",
    "if use_timing:\n",
    "    t1 = time.time()\n",
    "img, x = load_image(path=path,model_name=model_name)\n",
    "if use_timing:\n",
    "    t11=time.time()\n",
    "x=np.array(x)\n",
    "if use_timing:\n",
    "    t12=time.time()\n",
    "feature=img_to_conv_features(model_name=model_name,model=model,x=x)\n",
    "if use_timing:\n",
    "    t13=time.time()\n",
    "feature=np.array(feature)\n",
    "if use_timing:\n",
    "    t14=time.time()\n",
    "print(\"Shape of feature:\",np.shape(feature))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Caclulating dataset images (simulations) N.N.-fingerprint\n",
    "if use_timing:\n",
    "    t2 = time.time()\n",
    "if recalculate_dataset==True:\n",
    "    images=choose_imgs(max_num_images=max_num_images,images_path=image_path)\n",
    "if use_timing:\n",
    "    t3 = time.time()\n",
    "if recalculate_dataset==True:\n",
    "    features=imgs_to_conv_features(model_name=model_name,model=model,images=images,image_path=image_path)\n",
    "print(\"Shape of features:\",np.shape(features))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Possibly reduce dimensionality of data\n",
    "if use_timing:\n",
    "    t4 = time.time()\n",
    "if use_PCA==True:\n",
    "    full_features=np.vstack((feature,features))\n",
    "    reduced_full_features=reduce_PCA(full_features,n_components=n_components)\n",
    "    \n",
    "    reduced_feature=reduced_full_features[0,:]\n",
    "    reduced_features=reduced_full_features[1:,:]\n",
    "elif use_PCA==False:\n",
    "    reduced_features=features\n",
    "    reduced_feature=feature\n",
    "if use_timing:\n",
    "    t5 = time.time()    \n",
    "reduced_feature=np.array(reduced_feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate distances between target-dataset, and plot the best 5 matches\n",
    "if distance_metric=='knn':\n",
    "    distances,idx_closest=get_neighbors(feature_vector=reduced_feature,feature_vectors=reduced_features,k=k)\n",
    "    distances=distances.flatten()\n",
    "    idx_closest=idx_closest.flatten()\n",
    "    distances=np.array(distances)\n",
    "    target_image=path[-16:]\n",
    "    \n",
    "    print('\\n')\n",
    "    print('neighbour indeces with closest distances with respect to:%s'%target_image)\n",
    "    print('for neighbors with indexes respectively:')\n",
    "    print(idx_closest)\n",
    "    print('\\n')\n",
    "    i=0\n",
    "    for idx in idx_closest:\n",
    "        if i>=k:\n",
    "            print(\"i was found greater than k. Breaking the loop\")\n",
    "            break\n",
    "        i+=1\n",
    "        name=images[idx]\n",
    "        cut_name=name[-16:]\n",
    "        print('%d closest with distance %1.2f and name :%s'%(int(i),distances[i-1],cut_name))\n",
    "        \n",
    "       \n",
    "\n",
    "    if display_output:\n",
    "        query_image = img\n",
    "\n",
    "\n",
    "        results_image = get_concatenated_images(idx_closest, 200)\n",
    "\n",
    "        # display the query image\n",
    "        plt.figure(figsize = (5,5))\n",
    "        plt.imshow(query_image)\n",
    "\n",
    "        # display the resulting images\n",
    "        plt.figure(figsize = (16,12))\n",
    "\n",
    "        plt.imshow(results_image)\n",
    "        plt.title(\"result images\")\n",
    "if use_timing:\n",
    "    t6 = time.time()    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Timing outputs\n",
    "###########################################################################\n",
    "if use_timing:\n",
    "    Dt1=t1-t0\n",
    "    Dt2=t2-t1\n",
    "    Dt3=t3-t2\n",
    "    Dt4=t4-t3\n",
    "    Dt5=t5-t4\n",
    "    Dt6=t6-t5\n",
    "\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('Dt1=%3.3f s:Model parameter loading: '%Dt1)\n",
    "    \n",
    "    \n",
    "    print('%.2f percentage for loading image'%(t11-t1))\n",
    "    print('%.2f percentage for making image into a numpy array'%(t12-t11))\n",
    "    print('%.2f percentage for calculating convolutional features'%(t13-t12))\n",
    "    print('%.2f percentage for making features into a numpy array'%(t11-t1))\n",
    "    print('Dt2=%3.3f s:1) Image preprocessing and 2)loading of feature calculator 3) feature calculation for one image'%Dt2)\n",
    "    \n",
    "    \n",
    "    print('Dt3=%3.3f s:Choosing the images from the folder:'%Dt3)\n",
    "    print('Dt4=%3.3f s:Images to convolutional features for %d images'%(Dt4,max_num_images))\n",
    "    print('Dt5=%3.3f s:PCA '%Dt5)\n",
    "    print('Dt6=%3.3f s:K nearest neighbohrs'%Dt6)\n",
    "    print('\\n')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5y_fgURtJk4"
   },
   "source": [
    "### Just plotting the images on their own cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQ8IRjEAtJk5"
   },
   "outputs": [],
   "source": [
    "# path='C:\\\\Users\\\\Rami\\\\Desktop\\\\PetImages\\\\dogs-vs-cats\\\\cat_.jpg' \n",
    "path='/groups/astro/rami/images/image_000025.png'\n",
    "img, x = load_image(path=path,model_name=model_name)\n",
    "x=np.array(x)\n",
    "feature=img_to_conv_features(model_name=model_name,model=model,x=x)\n",
    "reduced_feature=np.array(feature)\n",
    "print(\"Shape of feature:\",np.shape(feature))\n",
    "\n",
    "\n",
    "if distance_metric=='knn':\n",
    "    distances,idx_closest=get_neighbors(feature_vector=reduced_feature,feature_vectors=reduced_features,k=k)\n",
    "    distances=distances.flatten()\n",
    "    idx_closest=idx_closest.flatten()\n",
    "    distances=np.array(distances)\n",
    "    target_image=path[-16:]\n",
    "    print('neighbour indeces with closest distances with respect to:%s'%target_image)\n",
    "    print(idx_closest)\n",
    "    i=0\n",
    "    for idx in idx_closest:\n",
    "        if i>=k:\n",
    "            print(\"i was found greater than k. Breaking the loop\")\n",
    "            break\n",
    "        i+=1\n",
    "        name=images[idx]\n",
    "        cut_name=name[-16:]\n",
    "        print('%d closest with distance %1.2f and name :%s'%(int(i),distances[i-1],cut_name))\n",
    "        \n",
    "\n",
    "    query_image = img\n",
    "    \n",
    "\n",
    "    results_image = get_concatenated_images(idx_closest, 200)\n",
    "    t6 = time.process_time()    \n",
    "\n",
    "    # display the query image\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(query_image)\n",
    "\n",
    "    # display the resulting images\n",
    "    plt.figure(figsize = (16,12))\n",
    "\n",
    "    plt.imshow(results_image)\n",
    "    plt.title(\"result images\")\n",
    "    plt.show()\n",
    "print(distance_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the  features to pickle_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # !mkdir features\n",
    "    # print(np.shape(images))\n",
    "    # print(np.shape(reduced_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# reduced_features=np.array(reduced_features)\n",
    "# for i in range(len(images)):\n",
    "#     name=images[i]\n",
    "#     folder_directory='features/'\n",
    "#     pickle_name=folder_directory+'feat_vector'+name[-10:-4]+'.pickle'\n",
    "#     #print(pickle_name)\n",
    "#     with open(pickle_name, 'wb') as f:\n",
    "#         save_features=reduced_features[i,:]\n",
    "#         pickle.dump(save_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read those pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# object = pd.read_pickle('features/feat_vector000654.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML4A_image-search.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow_gpu_kernel",
   "language": "python",
   "name": "tensorflow_gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
