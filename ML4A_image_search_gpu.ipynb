{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw96lUzwtJk1"
   },
   "source": [
    "# Feature extraction and reverse image search using pre-trained Deep Convolutional Neural Networks\n",
    "\n",
    "This notebook deals with the procedure of analyzing a large set of images using a pre-trained convolutional network, extracting feature vectors (activations of last layer) for each one which represent each image. \n",
    "\n",
    "After the analysis is done, we will review some retrieval tasks that you can do with such an analysis. The main task will be that of \"reverse image search,\" which refers to searching for the most similar set of images to some query image. \n",
    "\n",
    "\n",
    "### Step 0: Make a GPU-enabled environment (via the terminal, before runnning python code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) astro   2) python/anaconda3/2020.11   3) cuda/11.2\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's important to do pip install keras and not conda install keras because otherwise it would downgrade the tensorflow module.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!module list\n",
    "################################WITH CONDA\n",
    "\n",
    "#1 step create environment\n",
    "# conda -n gputest python=3\n",
    "\n",
    "#2 activate environment\n",
    "#conda activate gputest\n",
    "\n",
    "#3 install kernel for the gpu enbaling (?!)\n",
    "# pip install ipykernel\n",
    "\n",
    "#4 set kernel\n",
    "# python -m ipykernel install --user --name gputest --display-name \"gputest\"\n",
    "\n",
    "#5 get tensorflow for gpu running\n",
    "#conda install tensorflow-gpu\n",
    "\n",
    "#6 install jupyter environment\n",
    "#conda install jupyter\n",
    "\n",
    "# install keras library\n",
    "#pip install keras \n",
    "\n",
    "'''It's important to do pip install keras and \\\n",
    "not conda install keras because otherwise it would downgrade the \\\n",
    "tensorflow module.'''\n",
    "# 7 run the notebook\n",
    "#jupyter notebook\n",
    "\n",
    "\n",
    "#################################### WITH virtualenv\n",
    "\n",
    "\n",
    "#1  Build environment\n",
    "#virtualenv --system-site-packages targetDirectory \n",
    "#2 Activate environment\n",
    "# source ~/targetDirectory/bin/activate\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Import necessary modules and write main functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-29 22:22:57.272218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image \n",
    "import time\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "# config = tensorflow.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tensorflow.Session(config=config)\n",
    "\n",
    "\n",
    "\n",
    "def conv_model(model_name='resnet',show_model=True):\n",
    "    if model_name=='inception_v3': #works\n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        model = tensorflow.keras.applications.InceptionV3(weights='imagenet', include_top=True)\n",
    "    elif model_name=='vgg16':   #works \n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        model = tensorflow.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "    elif model_name=='xception': #works\n",
    "        from keras.applications.xception import Xception\n",
    "        model = tensorflow.keras.applications.Xception(weights='imagenet', include_top=True)\n",
    "    elif model_name=='resnet': #doesn't work\n",
    "        from keras.applications.resnet50 import ResNet50\n",
    "        model = tensorflow.keras.applications.ResNet50(weights='imagenet', include_top=True)\n",
    "    elif model_name=='vgg': #works\n",
    "        model = tensorflow.keras.applications.VGG19(weights='imagenet', include_top=True)\n",
    "    elif model_name=='MobileNet': #doesn't work\n",
    "        from keras.applications.mobilenet import MobileNet\n",
    "        model = tensorflow.keras.applications.MobileNet(weights='imagenet', include_top=True)\n",
    "    else:\n",
    "        print('Incorrect model_name fed to function conv_model')\n",
    "    if model!=None:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable=False\n",
    "    if show_model==True:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def delete_model(model, clear_session=True):\n",
    "    '''removes model!\n",
    "    '''\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if clear_session: K.clear_session()\n",
    "\n",
    "def get_neighbors(feature_vector,feature_vectors, k=5):\n",
    "    similar_idx = [ distance.euclidean(feature_vector, feat) for feat in feature_vectors ]\n",
    "    idx_closest = sorted(range(len(similar_idx)), key=lambda k: similar_idx[k])[1:1+k]\n",
    "    distances=[]\n",
    "    for i in idx_closest:\n",
    "        distances.append(similar_idx[i])\n",
    "    distances=np.array(distances)\n",
    "    idx_closest=np.array(idx_closest)\n",
    "    return distances,idx_closest        \n",
    "\n",
    "\n",
    "def load_image(path,model_name='resnet'):\n",
    "    img = image.load_img(path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    if model=='inception_v3':\n",
    "        from keras.applications.inception_v3 import preprocess_input as process_inception\n",
    "        x=preprocess_inception(x)\n",
    "    elif model_name=='vgg16':\n",
    "        from keras.applications.vgg16 import preprocess_input as process_vgg16\n",
    "        print('shape of vgg16 input:',np.shape(x))\n",
    "        x=process_vgg16(x)\n",
    "    elif model_name=='xception':\n",
    "        from keras.applications.xception import preprocess_input as process_xception\n",
    "        x=process_xception(x)\n",
    "    elif model_name=='resnet':\n",
    "        from keras_applications.resnet import preprocess_input as process_resnet\n",
    "        print('shape of resnet input:',np.shape(x))\n",
    "        x=process_resnet(x)\n",
    "    elif model_name=='vgg':\n",
    "        from keras.applications.vgg19 import preprocess_input as process_vgg\n",
    "        x=process_vgg(x)\n",
    "    elif model_name=='mobile':\n",
    "        from keras.applications.mobilenet import preprocess_input as process_mobile\n",
    "        x=process_mobile(x)  \n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function load_image')\n",
    "    return img, x\n",
    "\n",
    "def img_to_conv_features(model_name,model,x):\n",
    "    if model_name=='inception_v3':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='vgg16':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "        feat = feat_extractor.predict(x)\n",
    "    elif model_name=='xception':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='resnet':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "        feat = feat_extractor.predict(x)\n",
    "    elif model_name=='vgg':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function img_to_conv_features')\n",
    "    feat=feat_extractor(x)\n",
    "    return feat\n",
    "def imgs_to_conv_features(model_name,model,images,image_path):\n",
    "    if model_name=='inception_v3':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='vgg16':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    elif model_name=='xception':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
    "    elif model_name=='resnet':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    elif model_name=='vgg':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    elif model_name=='MobileNet':\n",
    "        feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "    else:\n",
    "        print('Incorrect  model_name fed to function img_to_conv_features')\n",
    "    tic = time.process_time()\n",
    "\n",
    "    every=20\n",
    "    features = []\n",
    "    for i, image_path in enumerate(images):\n",
    "\n",
    "        if i % every == 0:\n",
    "            toc = time.process_time()\n",
    "            elap = toc-tic;\n",
    "            remaining_time=(len(images)-i)*elap/every\n",
    "            hours=remaining_time//3600\n",
    "            minutes=remaining_time//60\n",
    "            seconds=remaining_time%60\n",
    "            print(\"analyzing image %d / %d. Time/%d pics: %4.4f seconds.\" % (i, len(images),every,elap))\n",
    "            print('Remaining time: %d hours %d minutes %d sec.' %(hours,minutes,seconds))\n",
    "            \n",
    "            tic = time.process_time()\n",
    "        img, x = load_image(path=image_path,model_name=model_name);\n",
    "        #print(image_path+\"\\n\")\n",
    "        feat = feat_extractor.predict(x)[0]\n",
    "        features.append(feat)\n",
    "\n",
    "    print('finished extracting features for %d images' % len(images))\n",
    "    return features\n",
    "\n",
    "\n",
    "def choose_imgs(max_num_images=2000,images_path='C:\\\\Users\\\\Rami\\\\Desktop\\\\PetImages\\\\dogs-vs-cats'):\n",
    "    image_extensions = ['.jpg', '.png', '.jpeg']   # case-insensitive (upper/lower doesn't matter)\n",
    "    images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(images_path) for f in filenames if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    print(np.shape(images))\n",
    "    if max_num_images < len(images):\n",
    "        images = [images[i] for i in sorted(random.sample(range(len(images)), max_num_images))]\n",
    "\n",
    "    print(\"keeping %d images to analyze\" % len(images))\n",
    "    return images\n",
    "def reduce_PCA(features,n_components=40):\n",
    "    features = np.array(features)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(features)\n",
    "    pca_features = pca.transform(features)\n",
    "    return pca_features\n",
    "\n",
    "def plot_activation_layer(feature_vector):\n",
    "    \n",
    "    print(np.shape(feature_vector))\n",
    "    feature_vector=np.array(feature_vector)\n",
    "    feature_vector=feature_vector.flatten()\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.ylabel('Activations of last layer (fc2)')\n",
    "    plt.xlabel('## of neuron')\n",
    "    plt.plot(feature_vector)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def get_concatenated_images(indexes, thumb_height):\n",
    "    thumbs = []\n",
    "    for idx in indexes:\n",
    "        img = image.load_img(images[idx])\n",
    "        img = img.resize((int(img.width * thumb_height / img.height), thumb_height))\n",
    "        thumbs.append(img)\n",
    "    concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n",
    "    return concat_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check version of tensorflow and Default running device (GPU OR CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# from distutils.version import LooseVersion\n",
    "# import warnings\n",
    "\n",
    "# assert LooseVersion(tensorflow.__version__)>=LooseVersion('1.0'), 'Please use Tensorflow'\n",
    "# print('TensorFlow Version: {}'.format(tensorflow.__version__))\n",
    "\n",
    "# #Check for gpu\n",
    "# if not tensorflow.test.gpu_device_name():\n",
    "#     warnings.warn('No GPU found. Please ensure you have installed  TensorFlow correctly')\n",
    "# else: \n",
    "#     print('Default GPU Device: {}'.format(tensorflow.test.gpu_device_name()))\n",
    "    \n",
    "# gpu_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs:\", len(gpu_devices))\n",
    "# cpu_devices = tensorflow.config.list_physical_devices('CPU')\n",
    "# print(\"Num CPUs:\", len(cpu_devices))\n",
    "\n",
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to set which image we want to compare to which folder of images. We can also choose the model and get some useful messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit about the context\n",
    "\n",
    "The networks that are implemented are some of the top performing on the classification contest by imagenet on classifying  \n",
    "pictures on 1000 labels , when fed with a dataset 1.2 milion pictures. \n",
    "Since the machine learning consists of \n",
    "\n",
    "\n",
    "Step 1: Designing the architechture of your network\n",
    "\n",
    "\n",
    "(how many layers, how many neurons (units) does a layer have, which\n",
    "connections exist between adjacent layers, and what is the mathematical operation that a layer operates on the input in order\n",
    "to produce the output (activation function) which wiill be fed to the next layer.\n",
    "\n",
    "Step 2: Train the model\n",
    "\n",
    "  Start by initializing some numbers for all weight (for example set them all to 0), and then try to find optimal values\n",
    "for these parameters, the weights between the connections. The basic idea on how to find these optimal values,\n",
    "is to define a loss function which represents  how well the programm is achieving what we want it to do. We want the loss\n",
    "function to be small when the program is accomplishing the task (classifying images with labels correctly)\n",
    "and we want the loss to be large when the program is failing (you give it a picture of a cat, and it tells you its a dog).\n",
    "Since the loss function of the CNN is dependant by some mathematic expression to the weights of the network, minimizing is\n",
    "an optimization problem which can be solved by just walking towards the negative derivative of L(W1,w2..,W_n).\n",
    "A common algorithm that does this downhill walk on ths multidimensional space of weights in order\n",
    "to find the optimal values of the weights is stohastic gradient descent (SGD)\n",
    "\n",
    "\n",
    "Step 3: Your model is trained now and has small loss. \n",
    "\n",
    "Now the cnn can receive new input images and be able to produce \"signature-information\" containing feature vectors.\n",
    "\n",
    "\n",
    "Propagate the input x in the network and return the second-last layer, commonly called feature vector (denamed by \n",
    "  variable feature here). The feature vector has ~10^3 dimensions and simply consists of all the weight between the last 2 \n",
    "layers.  It is common in convolutional neural network (CNN) architecture that the two last layers are fully\n",
    "conected, while the rest of the layers inside the convolutional network (convolutions,max pool, avg pool, etc.) can skip\n",
    "connections from a layer to the next one.\n",
    "\n",
    "\n",
    "Remember that the networks that we are implementing have very high accuracy on classifying 1000 different types of objects\n",
    "which means that the layer before the last inputs some vector and outputs it's activation function to the last layer,\n",
    "which is the 1000 element layer of the predictions. Because of this, the feature vector is a better representation of the \n",
    "image's content than simply the 2D X3 (rgb) matrix of the colors consisting the image in order to assess a similarity measure.\n",
    "\n",
    "\n",
    "We dont use the last layer of the network (predictions) but the second last (feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to find the closest pictures could also be to say: let's find the pictures in the dataset which are the closest to our own by distance on their vector space. These ones with the least distance will be classified as best matches for the reverse image search. We will use the 5-nearest-neighbours algorithm, which in general for k neighbours is called kNN (k nearest neighbours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/astro/rami/tensorflow_gpu\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the neural network\n",
    "### All  the code in one cell without many outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "##########################################################\n",
    "path='/groups/astro/rami/images/image_000005.png'        #\n",
    "image_path='/groups/astro/rami/images/'                  #\n",
    "max_num_images=1500                                       #\n",
    "model_name='vgg'                                         #\n",
    "distance_metric='knn'                                    #\n",
    "k=5                                                      #\n",
    "use_PCA=False                                            # \n",
    "n_components=500                                         #\n",
    "recalculate_dataset=True                                #\n",
    "###########################################################\n",
    "print(\"Running search with image:%s \\n comparing to %d images \\n which are in folder %s, \\n using model %s \\n and distance metric:\\\n",
    "%s with %d nearest neighbors. \\n PCA used?:%s \\n recalculating dataset?%s \"%(path,max_num_images,image_path,model_name,distance_metric,k,use_PCA,recalculate_dataset))\n",
    "\n",
    "\n",
    "### Initializing model\n",
    "t0 = time.time()\n",
    "model=conv_model(model_name=model_name)  \n",
    "\n",
    "\n",
    "# Calculating target image (observation) N.N.-fingerprint\n",
    "t1 = time.time()\n",
    "img, x = load_image(path=path,model_name=model_name)\n",
    "x=np.array(x)\n",
    "feature=img_to_conv_features(model_name=model_name,model=model,x=x)\n",
    "feature=np.array(feature)\n",
    "print(\"Shape of feature:\",np.shape(feature))\n",
    "\n",
    "\n",
    "# Caclulating dataset images (simulations) N.N.-fingerprint\n",
    "t2 = time.time()\n",
    "if recalculate_dataset==True:\n",
    "    images=choose_imgs(max_num_images=max_num_images,images_path=image_path)\n",
    "t3 = time.time()\n",
    "if recalculate_dataset==True:\n",
    "    features=imgs_to_conv_features(model_name=model_name,model=model,images=images,image_path=image_path)\n",
    "print(\"Shape of features:\",np.shape(features))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Possibly reducec dimensionality of data\n",
    "t4 = time.time()\n",
    "if use_PCA==True:\n",
    "    full_features=np.vstack((feature,features))\n",
    "    reduced_full_features=reduce_PCA(features,n_components=n_components)\n",
    "    \n",
    "    reduced_feature=reduced_full_features[0,:]\n",
    "    reduced_features=reduced_full_features[1:,:]\n",
    "elif use_PCA==False:\n",
    "    reduced_features=features\n",
    "    reduced_feature=feature\n",
    "t5 = time.time()    \n",
    "reduced_feature=np.array(reduced_feature)\n",
    "\n",
    "\n",
    "# Calculate distances between target-dataset, and plot the best 5 matches\n",
    "if distance_metric=='knn':\n",
    "    distances,idx_closest=get_neighbors(feature_vector=reduced_feature,feature_vectors=reduced_features,k=k)\n",
    "    distances=distances.flatten()\n",
    "    idx_closest=idx_closest.flatten()\n",
    "    distances=np.array(distances)\n",
    "    target_image=path[-16:]\n",
    "    \n",
    "    print('\\n')\n",
    "    print('neighbour indeces with closest distances with respect to:%s'%target_image)\n",
    "    print('for neighbors with indexes respectively:')\n",
    "    print(idx_closest)\n",
    "    print('\\n')\n",
    "    i=0\n",
    "    for idx in idx_closest:\n",
    "        if i>=k:\n",
    "            print(\"i was found greater than k. Breaking the loop\")\n",
    "            break\n",
    "        i+=1\n",
    "        name=images[idx]\n",
    "        cut_name=name[-16:]\n",
    "        print('%d closest with distance %1.2f and name :%s'%(int(i),distances[i-1],cut_name))\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "    query_image = img\n",
    "    \n",
    "\n",
    "    results_image = get_concatenated_images(idx_closest, 200)\n",
    "    \n",
    "    # display the query image\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(query_image)\n",
    "\n",
    "    # display the resulting images\n",
    "    plt.figure(figsize = (16,12))\n",
    "\n",
    "    plt.imshow(results_image)\n",
    "    plt.title(\"result images\")\n",
    "t6 = time.time()    \n",
    "\n",
    "\n",
    "# Timing outputs\n",
    "###########################################################################\n",
    "Dt1=t1-t0\n",
    "Dt2=t2-t1\n",
    "Dt3=t3-t2\n",
    "Dt4=t4-t3\n",
    "Dt5=t5-t4\n",
    "Dt6=t6-t5\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Dt1=%3.3f s:Model parameter loading: '%Dt1)\n",
    "print('Dt2=%3.3f s:1 Image preprocessing and feature calculation:'%Dt2)\n",
    "print('Dt3=%3.3f s:Choosing the images from the folder:'%Dt3)\n",
    "print('Dt4=%3.3f s:Images to convolutional features'%Dt4)\n",
    "print('Dt5=%3.3f s:PCA '%Dt5)\n",
    "print('Dt6=%3.3f s:K nearest neighbohrs'%Dt6)\n",
    "print('\\n')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5y_fgURtJk4"
   },
   "source": [
    "### Just plotting the images on their own cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQ8IRjEAtJk5"
   },
   "outputs": [],
   "source": [
    "# path='C:\\\\Users\\\\Rami\\\\Desktop\\\\PetImages\\\\dogs-vs-cats\\\\cat_.jpg' \n",
    "path='/groups/astro/rami/images/image_000025.png'\n",
    "img, x = load_image(path=path,model_name=model_name)\n",
    "x=np.array(x)\n",
    "feature=img_to_conv_features(model_name=model_name,model=model,x=x)\n",
    "reduced_feature=np.array(feature)\n",
    "print(\"Shape of feature:\",np.shape(feature))\n",
    "\n",
    "\n",
    "if distance_metric=='knn':\n",
    "    distances,idx_closest=get_neighbors(feature_vector=reduced_feature,feature_vectors=reduced_features,k=k)\n",
    "    distances=distances.flatten()\n",
    "    idx_closest=idx_closest.flatten()\n",
    "    distances=np.array(distances)\n",
    "    target_image=path[-16:]\n",
    "    print('neighbour indeces with closest distances with respect to:%s'%target_image)\n",
    "    print(idx_closest)\n",
    "    i=0\n",
    "    for idx in idx_closest:\n",
    "        if i>=k:\n",
    "            print(\"i was found greater than k. Breaking the loop\")\n",
    "            break\n",
    "        i+=1\n",
    "        name=images[idx]\n",
    "        cut_name=name[-16:]\n",
    "        print('%d closest with distance %1.2f and name :%s'%(int(i),distances[i-1],cut_name))\n",
    "        \n",
    "\n",
    "    query_image = img\n",
    "    \n",
    "\n",
    "    results_image = get_concatenated_images(idx_closest, 200)\n",
    "    t6 = time.process_time()    \n",
    "\n",
    "    # display the query image\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(query_image)\n",
    "\n",
    "    # display the resulting images\n",
    "    plt.figure(figsize = (16,12))\n",
    "\n",
    "    plt.imshow(results_image)\n",
    "    plt.title(\"result images\")\n",
    "    plt.show()\n",
    "print(distance_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the  features to pickle_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir features\n",
    "print(np.shape(images))\n",
    "print(np.shape(reduced_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "reduced_features=np.array(reduced_features)\n",
    "for i in range(len(images)):\n",
    "    name=images[i]\n",
    "    folder_directory='features/'\n",
    "    pickle_name=folder_directory+'feat_vector'+name[-10:-4]+'.pickle'\n",
    "    #print(pickle_name)\n",
    "    with open(pickle_name, 'wb') as f:\n",
    "        save_features=reduced_features[i,:]\n",
    "        pickle.dump(save_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read those pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "object = pd.read_pickle('features/feat_vector000654.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML4A_image-search.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow_gpu_kernel",
   "language": "python",
   "name": "tensorflow_gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
